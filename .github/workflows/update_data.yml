name: Update Dataset Weekly

on:
  schedule:
    - cron: '0 17 * * 1'  # Every Monday 17:00 UTC
  workflow_dispatch:        # Manual trigger

# FBref requires undetected-chromedriver (Cloudflare) so it runs locally.
# CI only: downloads FBref CSV from S3, scrapes Understat, merges, uploads.
# To update FBref data: run `python scraper/fbref_scraper.py` locally,
# then `python scraper/upload_to_s3.py` to push fbref_players.csv to S3.

jobs:
  scrape-and-update:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      # ── Install dependencies ──
      - name: Install Python packages
        run: pip install pandas numpy boto3 lxml html5lib playwright

      - name: Install Playwright Chromium
        run: playwright install --with-deps chromium

      - name: Create data directory
        run: mkdir -p data

      # ── Download FBref data from S3 ──
      - name: Download FBref CSV from S3
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: eu-central-1
        run: |
          python -c "
          import boto3
          s3 = boto3.client('s3')
          s3.download_file('football-alpha-analysis-doruk', 'raw/fbref_players.csv', 'data/fbref_players.csv')
          import pandas as pd
          df = pd.read_csv('data/fbref_players.csv')
          print(f'[OK] FBref data downloaded: {len(df)} players, {len(df.columns)} columns')
          "

      # ── Scrape Understat ──
      - name: Scrape Understat (xG data)
        working-directory: scraper
        run: python understat_scraper.py

      # ── Verify scraped data ──
      - name: Verify data files
        run: |
          if [ ! -f data/fbref_players.csv ]; then
            echo "[ERROR] fbref_players.csv missing"
            exit 1
          fi
          if [ ! -f data/understat_players.csv ]; then
            echo "[ERROR] understat_players.csv missing"
            exit 1
          fi
          echo "[OK] Both source files present"
          wc -l data/fbref_players.csv data/understat_players.csv

      # ── Merge datasets ──
      - name: Merge FBref + Understat
        working-directory: scraper
        run: python merge_data.py

      # ── Verify merged data ──
      - name: Verify merged file
        run: |
          python -c "
          import pandas as pd
          df = pd.read_csv('data/players_data.csv')
          print(f'Players: {len(df)}')
          print(f'Columns: {len(df.columns)}')
          print(f'xG coverage: {df[\"xg\"].notna().sum()}/{len(df)}')
          assert len(df) > 2000, f'Too few players: {len(df)}'
          print('[OK] Merge verified')
          "

      # ── Upload to S3 ──
      - name: Upload to S3
        working-directory: scraper
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: eu-central-1
        run: python upload_to_s3.py