name: Update Dataset Weekly

on:
  schedule:
    - cron: '0 17 * * 1'
  workflow_dispatch:

jobs:
  update:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: pip install pandas numpy requests beautifulsoup4 boto3 lxml

      - name: Download existing FBref data from S3
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: eu-central-1
        run: |
          mkdir -p data
          python -c "
          import boto3
          s3 = boto3.client('s3')
          try:
              s3.download_file('football-alpha-analysis-doruk', 'data/fbref_players.csv', 'data/fbref_players.csv')
              print('[OK] FBref data downloaded')
          except Exception as e:
              print(f'[WARN] {e}')
          "

      - name: Scrape Understat
        working-directory: scraper
        run: python understat_scraper.py

      - name: Merge datasets
        working-directory: scraper
        run: python merge_data.py

      - name: Upload to S3
        working-directory: scraper
        run: python upload_to_s3.py